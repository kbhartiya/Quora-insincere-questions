{"cells":[{"metadata":{"trusted":true,"_uuid":"09aa1f8df00fa54dec74987cbfbe5cb9fa85ca40"},"cell_type":"code","source":"from keras.layers import K, Activation, LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, LSTM\nfrom keras.engine import Layer\nfrom keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f06eb8301e3ed14012454426be20d01c0fc3e895"},"cell_type":"code","source":"#! -*- coding: utf-8 -*-\n# refer: https://kexue.fm/archives/5112\n\nfrom keras import activations\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\n\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n#define our own softmax function instead of K.softmax\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex/K.sum(ex, axis=axis, keepdims=True)\n\n\n#A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = K.batch_dot(c, u_hat_vecs, [2, 2])\n            if K.backend() == 'theano':\n                o = K.sum(o, axis=1)\n            if i < self.routings - 1:\n                o = K.l2_normalize(o, -1)\n                b = K.batch_dot(o, u_hat_vecs, [2, 3])\n                if K.backend() == 'theano':\n                    b = K.sum(b, axis=1)\n\n        return self.activation(o)\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c670c24ed16b5ce666c9172839ad5c2f5a67bbe2"},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nos.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d343fec8d71bff544769f5074079a4e1d09582e"},"cell_type":"code","source":"lstm_len = 256\ngru_len = 256\nRoutings = 3\nNum_capsule = 10\nDim_capsule = 10\ndropout_p = 0.25\nrate_drop_dense = 0.28\n\nmax_features = 20000\nmaxlen = 135\nembed_size = 256\nbatch_size = 100\nn_epochs = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde8a5ca625400f40927c977a89b6e308c6104f8"},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"751d0ff3ab2877ee0d5be85203b6664c185add35"},"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0e8cb2f1e43fdf6eddca9160b2ee9a3fbb25b4a2"},"cell_type":"code","source":"tokenizer.fit_on_texts(df['question_text'].values)\nx = tokenizer.texts_to_sequences(df['question_text'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c675357472a7ceb0df8ab0477681f30978957722"},"cell_type":"code","source":"x = sequence.pad_sequences(x,maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e6a2d70cb33e117ecee22b9414dc42ba3c8d4ab"},"cell_type":"code","source":"'''\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[len(line)-300:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\n    '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8bc6d889cfb47b49b8ba10820df911e9c591fe4"},"cell_type":"code","source":"#word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"179172098d363140d1f4115f2c1b45aceb158d2e"},"cell_type":"code","source":"def sentences_to_indices(X, word_to_index, max_len):\n    \"\"\"\n    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n    \n    Arguments:\n    X -- array of sentences (strings), of shape (m, 1)\n    word_to_index -- a dictionary containing the each word mapped to its index\n    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n    \n    Returns:\n    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n    \"\"\"\n    \n    m = X.shape[0]                                   # number of training examples\n    \n    ### START CODE HERE ###\n    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n    X_indices = np.zeros((m,max_len))\n    \n    for i in range(m):                               # loop over training examples\n        \n        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n        sentence_words = X[i].lower().split()\n        \n        # Initialize j to 0\n        j = 0\n        \n        # Loop over the words of sentence_words\n        for w in sentence_words:\n            # Set the (i,j)th entry of X_indices to the index of the correct word.\n            X_indices[i, j] = word_to_index[w]\n            # Increment j to j + 1\n            j = j+1\n            \n    ### END CODE HERE ###\n    \n    return X_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"224a7603f65d28a1b160ba12b673b9b61c465f7b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,df['target'],random_state=101,test_size=0.50)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dd2bf5c5227ec7502fc6361b7efd218e2ec0d78"},"cell_type":"code","source":"#https://github.com/stefan-it/capsnet-nlp\ndef get_model():\n    input1 = Input(shape=(maxlen,))\n    embed_layer = Embedding(max_features,\n                            embed_size,\n                            input_length=maxlen)(input1)\n    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n\n    x = Bidirectional(LSTM(lstm_len,\n                          activation='relu',\n                          dropout=dropout_p,\n                          recurrent_dropout=dropout_p,\n                          return_sequences=True))(embed_layer)\n    capsule = Capsule(\n        num_capsule=Num_capsule,\n        dim_capsule=Dim_capsule,\n        routings=Routings,\n        share_weights=True)(x)\n\n    capsule = Flatten()(capsule)\n    capsule = Dropout(dropout_p)(capsule)\n    capsule = LeakyReLU()(capsule)\n\n    x = Flatten()(x)\n    output = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=input1, outputs=output)\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'])\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07a79a4ff076bf8e855a492fcd3890ee4d585334"},"cell_type":"code","source":"model = get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a99d9e85ad39aa0e4a439c00fafc1b9b42b725d"},"cell_type":"code","source":"model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs,validation_data=(X_test, y_test),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef98e9d538e63ef7a9168c2c834605c569c68784"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e3b7722d03970e824c7bd42be453583d1157690"},"cell_type":"code","source":"testx = tokenizer.texts_to_sequences(test_df['question_text'].values)\nX_test = sequence.pad_sequences(testx,maxlen=135)\n\nall_preds = []\ny_pred = model.predict(X_test,batch_size=128,verbose=1)\nall_preds.append(y_pred.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41920203934eb4729cc9ff1758d67f7815da982f"},"cell_type":"code","source":"print(y_pred.shape)\ny_te = (np.array(y_pred.flatten()) > 0.35).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}